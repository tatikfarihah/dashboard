{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AGZtwpxLWfM-"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk membuat dataset dengan look_back\n",
        "def create_dataset(X, Y, look_back=1):\n",
        "    Xs, Ys = [], []\n",
        "    for i in range(len(X) - look_back - 1):\n",
        "        Xs.append(X[i:(i + look_back)])\n",
        "        Ys.append(Y[i + look_back])\n",
        "    return np.array(Xs), np.array(Ys)\n",
        "\n",
        "# Model LSTM\n",
        "def build_lstm_model(input_shape, units, dropout):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(LSTM(units))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load data\n",
        "df = pd.read_excel(\"DATA FIKS.xlsx\")\n",
        "dataY = df['Price'].values.reshape(-1, 1)\n",
        "dataX = df.drop(['Date', 'Price'], axis=1).values  # drop columns you don't want\n",
        "feature_names = df.drop(['Date', 'Price'], axis=1).columns.tolist()\n",
        "\n",
        "# Standarisasi data\n",
        "scalerX, scalerY = StandardScaler(), StandardScaler()\n",
        "dataX = scalerX.fit_transform(dataX)\n",
        "dataY = scalerY.fit_transform(dataY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Buat model\n",
        "look_back = 1\n",
        "train_size = int(len(dataX) * 0.8)\n",
        "trainX, testX = dataX[:train_size], dataX[train_size:]\n",
        "trainY, testY = dataY[:train_size], dataY[train_size:]\n",
        "\n",
        "trainX, trainY = create_dataset(trainX, trainY, look_back)\n",
        "testX, testY = create_dataset(testX, testY, look_back)\n",
        "\n",
        "# Bentuk ulang input menjadi 3D\n",
        "trainX = trainX.reshape(trainX.shape[0], look_back, trainX.shape[2])\n",
        "testX = testX.reshape(testX.shape[0], look_back, testX.shape[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0.]]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "X has 2 features, but StandardScaler is expecting 5 features as input.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m next_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([harga_btc_t1, volume_btc])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(next_input)\n\u001b[1;32m---> 67\u001b[0m next_input_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscalerX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_input\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, look_back, next_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Prediksi 1 hari ke depan\u001b[39;00m\n\u001b[0;32m     70\u001b[0m next_prediction_scaled \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(next_input_scaled)\n",
            "File \u001b[1;32mc:\\Users\\Ariq\\anaconda3\\envs\\dashboard\\lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\Ariq\\anaconda3\\envs\\dashboard\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
            "File \u001b[1;32mc:\\Users\\Ariq\\anaconda3\\envs\\dashboard\\lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\Ariq\\anaconda3\\envs\\dashboard\\lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: X has 2 features, but StandardScaler is expecting 5 features as input."
          ]
        }
      ],
      "source": [
        "\n",
        "# Dashboard menggunakan Streamlit\n",
        "st.title(\"Prediksi Harga Bitcoin\")\n",
        "\n",
        "# Pilihan variabel input dari pengguna\n",
        "selected_features = st.multiselect('Pilih variabel input', feature_names)\n",
        "\n",
        "# Jika tidak ada variabel yang dipilih, berikan pesan\n",
        "if len(selected_features) == 0:\n",
        "    st.write(\"Silakan pilih setidaknya satu variabel untuk memulai prediksi.\")\n",
        "\n",
        "else:\n",
        "    # Filter data sesuai dengan fitur yang dipilih\n",
        "    selected_indices = [feature_names.index(f) for f in selected_features]\n",
        "\n",
        "    # Update dataX sesuai variabel yang dipilih\n",
        "    dataX_selected = df[selected_features].values\n",
        "    dataX_selected_scaled = scalerX.fit_transform(dataX_selected)\n",
        "\n",
        "    # Bagi data ke dalam train/test\n",
        "    trainX_selected, testX_selected = dataX_selected_scaled[:train_size], dataX_selected_scaled[train_size:]\n",
        "    trainX_selected, trainY_selected = create_dataset(trainX_selected, dataY[:train_size], look_back)\n",
        "    testX_selected, testY_selected = create_dataset(testX_selected, dataY[train_size:], look_back)\n",
        "\n",
        "    trainX_selected = trainX_selected.reshape(trainX_selected.shape[0], look_back, trainX_selected.shape[2])\n",
        "    testX_selected = testX_selected.reshape(testX_selected.shape[0], look_back, testX_selected.shape[2])\n",
        "\n",
        "    # Buat model\n",
        "    model = build_lstm_model((look_back, trainX_selected.shape[2]), units=100, dropout=0.3)\n",
        "    model.fit(trainX_selected, trainY_selected, epochs=50, batch_size=16, verbose=0, shuffle=False)\n",
        "\n",
        "    # Prediksi menggunakan data testing\n",
        "    testPredict_scaled = model.predict(testX_selected)\n",
        "    testPredict = scalerY.inverse_transform(testPredict_scaled)\n",
        "    testY_actual = scalerY.inverse_transform(testY_selected)\n",
        "\n",
        "    # Hitung MAPE\n",
        "    test_mape = mean_absolute_percentage_error(testY_actual, testPredict)\n",
        "    st.write(f\"Loss (MAPE) pada data testing: {test_mape:.2f}\")\n",
        "\n",
        "    # Plot hasil prediksi vs aktual\n",
        "    st.subheader(\"Grafik Prediksi vs Data Aktual\")\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(testY_actual, label=\"Data Aktual\")\n",
        "    ax.plot(testPredict, label=\"Prediksi\")\n",
        "    ax.legend()\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # SHAP interpretation berdasarkan data testing\n",
        "    st.subheader(\"Grafik SHAP untuk Interpretasi Model pada Data Testing\")\n",
        "    explainer = shap.KernelExplainer(model.predict, testX_selected)\n",
        "    shap_values = explainer.shap_values(testX_selected)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    shap.summary_plot(shap_values, feature_names=selected_features, plot_type=\"bar\", show=False)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "# Prediksi 1 periode ke depan berdasarkan harga dan volume perdagangan\n",
        "st.subheader(\"Prediksi 1 Hari ke Depan\")\n",
        "\n",
        "# Input pengguna untuk prediksi 1 hari ke depan\n",
        "harga_btc_t1 = st.number_input('Harga Bitcoin t-1', value=0.0, format=\"%.2f\")\n",
        "volume_btc = st.number_input('Volume Perdagangan Bitcoin', value=0.0, format=\"%.2f\")\n",
        "\n",
        "# Bentuk array input untuk prediksi 1 hari ke depan\n",
        "next_input = np.array([harga_btc_t1, volume_btc]).reshape(1, -1)\n",
        "print(next_input)\n",
        "next_input_scaled = scalerX.transform(next_input).reshape(1, look_back, next_input.shape[1])\n",
        "\n",
        "# Prediksi 1 hari ke depan\n",
        "next_prediction_scaled = model.predict(next_input_scaled)\n",
        "next_prediction = scalerY.inverse_transform(next_prediction_scaled)\n",
        "\n",
        "st.write(f\"Prediksi Harga Bitcoin 1 Hari ke Depan: {next_prediction[0][0]:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dashboard",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
